{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import transformers as tfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='allenai/scibert_scivocab_cased', vocab_size=31116, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tfs.AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_cased\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,   197, 18814, 17597,   163,   105,  4257,   188,  1745, 27604,\n",
       "         30111,   430,   955,   578,  4257,  5240,   102],\n",
       "        [  101,   568,  1915, 30108,   163,   105,  5649,   188,  1745, 27604,\n",
       "         30111,   211,   102,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs = tokenizer(\n",
    "    [\n",
    "        \"Alabaster is a word with multiple syllables, multi-word dataset\".split(), \n",
    "        \"America is a country with multiple syllables.\".split(),\n",
    "    ], \n",
    "    return_tensors=\"pt\", \n",
    "    padding=True, \n",
    "    truncation=True, \n",
    "    max_length=512,\n",
    "    is_split_into_words=True\n",
    ")\n",
    "outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[CLS]',\n",
       "  'al',\n",
       "  '##aba',\n",
       "  '##ster',\n",
       "  'is',\n",
       "  'a',\n",
       "  'word',\n",
       "  'with',\n",
       "  'multiple',\n",
       "  'syllable',\n",
       "  '##s',\n",
       "  ',',\n",
       "  'multi',\n",
       "  '-',\n",
       "  'word',\n",
       "  'dataset',\n",
       "  '[SEP]'],\n",
       " ['[CLS]',\n",
       "  'am',\n",
       "  '##eric',\n",
       "  '##a',\n",
       "  'is',\n",
       "  'a',\n",
       "  'country',\n",
       "  'with',\n",
       "  'multiple',\n",
       "  'syllable',\n",
       "  '##s',\n",
       "  '.',\n",
       "  '[SEP]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]']]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(\n",
    "    tokenizer.convert_ids_to_tokens,\n",
    "    outs.input_ids\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' alabaster is a word with multiple syllables, multi - word dataset '"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from typing import List\n",
    "\n",
    "def replace_chain(text:str, tokens_to_drop:List[str]) -> str:\n",
    "    return reduce(\n",
    "        lambda t, token: t.replace(token, \"\"), \n",
    "        tokens_to_drop, \n",
    "        text\n",
    "    )\n",
    "\n",
    "def detokenize(tokenizer, input_ids):\n",
    "    text = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    )\n",
    "\n",
    "    return replace_chain(text, [\"[CLS]\", \"[SEP]\", \"[PAD]\"])\n",
    "\n",
    "\n",
    "detokenize(tokenizer, outs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alabaster is a word with multiple syllables.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outs[\"input_ids\"][0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'al',\n",
       " '##aba',\n",
       " '##ster',\n",
       " 'is',\n",
       " 'a',\n",
       " 'word',\n",
       " 'with',\n",
       " 'multiple',\n",
       " 'syllable',\n",
       " '##s',\n",
       " ',',\n",
       " 'multi',\n",
       " '-',\n",
       " 'word',\n",
       " 'dataset',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = tokenizer.convert_ids_to_tokens(outs[\"input_ids\"][0])\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_classification = np.array(\n",
    "    [0.8, 0.9, 0.1, 0.5, 0.2, 0.3, 0.4, 0.5, 0.7, 0.1, 0.1, 0.1, 0.95, 0.93, 0.9, 0.9, 0.95],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]: 0.8\n",
      "al: 0.9\n",
      "##aba: 0.1\n",
      "##ster: 0.5\n",
      "is: 0.2\n",
      "a: 0.3\n",
      "word: 0.4\n",
      "with: 0.5\n",
      "multiple: 0.7\n",
      "syllable: 0.1\n",
      "##s: 0.1\n",
      ",: 0.1\n",
      "multi: 0.95\n",
      "-: 0.93\n",
      "word: 0.9\n",
      "dataset: 0.9\n",
      "[SEP]: 0.95\n"
     ]
    }
   ],
   "source": [
    "for c, t in zip(example_classification, example):\n",
    "    print(f\"{t}: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('alabaster', 0.9)],\n",
       " [('multi', 0.95), ('-', 0.93), ('word', 0.9), ('dataset', 0.9)]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import filterfalse\n",
    "from typing import Tuple\n",
    "from typing import Tuple\n",
    "from itertools import takewhile\n",
    "\n",
    "\n",
    "def merge_tokens_w_classifications(\n",
    "    tokens:List[str], \n",
    "    classifications:List[float]\n",
    ") -> List[Tuple[str, float]]:\n",
    "    merged = []\n",
    "    for token, classification in zip(tokens, classifications):\n",
    "        if token.startswith(\"##\"):\n",
    "            merged[-1] = (merged[-1][0] + token[2:], merged[-1][1])\n",
    "        else:\n",
    "            merged.append((token, classification))\n",
    "    return merged\n",
    "\n",
    "\n",
    "def is_special_token(token):\n",
    "    return token.startswith(\"[\") and token.endswith(\"]\")\n",
    "\n",
    "\n",
    "def high_probablity_token_groups(\n",
    "    tokens_classifications: List[Tuple[str, float]], \n",
    "    threshold=0.5,\n",
    ") -> List[List[Tuple[str, float]]]:\n",
    "\n",
    "    datasets = []\n",
    "    dataset = []\n",
    "    for token, score in tokens_classifications:\n",
    "        if score >= threshold:\n",
    "            dataset.append((token, score))\n",
    "        else:\n",
    "            if len(dataset) > 0:\n",
    "                datasets.append(dataset)\n",
    "                dataset = []\n",
    "    if len(dataset) > 0:\n",
    "        datasets.append(dataset)\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "tokens_classifications = list(filterfalse(\n",
    "    lambda x: is_special_token(x[0]),\n",
    "    merge_tokens_w_classifications(example, example_classification)\n",
    "))\n",
    "high_probablity_token_groups(tokens_classifications, threshold=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('alabaster', 0.9),\n",
       " ('is', 0.2),\n",
       " ('a', 0.3),\n",
       " ('word', 0.4),\n",
       " ('with', 0.5),\n",
       " ('multiple', 0.7),\n",
       " ('syllables', 0.1),\n",
       " (',', 0.1),\n",
       " ('multi', 0.95),\n",
       " ('-', 0.93),\n",
       " ('word', 0.9),\n",
       " ('dataset', 0.9)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 []\n",
      "1 []\n",
      "0 []\n"
     ]
    }
   ],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "\n",
    "for k, g in  list(groupby(tokens_classifications, lambda x: int(x[1]>0.9))):\n",
    "    print(k, list(g))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('alabaster', 0.9)],\n",
       " [('multi', 0.95), ('-', 0.93), ('word', 0.9), ('dataset', 0.9)]]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "high_probablity_token_groups(tokens_classifications, threshold=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 4)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1, 2, 3, 4]])[np.newaxis, ...].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('democratizingdata-ml-algo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b3e03aa19f4cee1cca181f326aea7726af1ea307b5fcbfd54a41f945b089b370"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
